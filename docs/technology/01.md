---
title: iOS上用python爬小说/图片
date: 2020-01-21
categories: article
author: Kamchan
tags:
  - 技术
  - IOS
  - App
  - Python
  - 小说
---

## 一、app 的推荐

过去，ios 上多使用 pythonista 这个软件来运行脚本。

但是，这个 app 全英文，好几十块钱，

最关键的是作者一年年的不更新，

基本已经是老古董了。

这里我个人推荐一款 app（有更好的选择欢迎留言告诉我和小伙伴们），

那就是[python ai](https://apps.apple.com/cn/app/id1471351733)。

![python ai](https://kamchan.oss-cn-shenzhen.aliyuncs.com/technology/python/python-ai.png)

:::tip 优点
各种模块很新 ，国区就能下载 ，免费软件 ，全中文界面
:::

:::warning 缺点
不能添加到系统默认的分享列表； 不能从 app 内直接打开下载资源的存储目录
:::

## 二、如何导入脚本？

首先，找到别人写好的脚本，比如今天的例子：

climbingFiction.py

```python
import re
import os
import lxml
import random
import requests
from bs4 import BeautifulSoup

###################### 全局变量 ##############################
book_url = ''
ua = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:70.0) Gecko/20100101 Firefox/70.0',
      'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1',
      'Opera/9.80 (Windows NT 6.1; U; zh-cn) Presto/2.9.168 Version/11.50',
      'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36',
      'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; .NET4.0C; .NET4.0E)'
      ]
Usera = random.choice(ua)
headers = {'User-Agent':Usera,
           'Accept': "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
           'Accept-Encoding': 'gzip',
           "Referer": "https://www.bqg3.com/quanbuxiaoshuo/"}
##############################################################
def get(url):
    a = requests.get(url, headers=headers)
    a.encoding= 'gb18030'
    html = a.text
    return html

def main(book_name):
    find_book(book_name)
    if book_url != '':

        print('正在获取小说章节信息...')
        mulu  = BeautifulSoup(get(book_url), 'lxml')
        all_zj = mulu.find('dl').find_all('a')


        for j in all_zj:
            mulu_list = [j.get('href')]
            zj_name = j.contents[0]
            zj_name = str(re.findall(r'\w*',zj_name))
            zj_name = zj_name.replace(("'"),'')
            zj_name = zj_name.replace((" "),'')
            zj_name = zj_name.replace((","),'')
            for k in mulu_list:
                zj_url = 'https://www.bqg3.com' + k
                a = (zj_name,zj_url)
                down(a)


def down(text_url):

    zj_name = text_url[0]
    txt_url = text_url[1]

    soup = BeautifulSoup(get(txt_url), 'lxml')
    zz = r'<div id="content">(.*?)</div>'
    text = re.findall(zz, str(soup.contents[1]), re.S)
    text = text[0].replace('<br/>', '')
    text = text[30:-len(book_url)]
    try:
        with open(book_name + '.txt', 'a+',encoding='utf-8')as f:
            print('下载小说章节%s...' % zj_name, txt_url)
            f.write('\n'+zj_name+'\n\n'+text+'\n')
    except Exception as e:
        print('错误信息: ',e)


def new_dir(name):
    if os.path.exists(name):
        print('"%s"  文件夹已存在'%name)
        os.chdir(name)
    else:
        print('创建文件夹: {}'.format(name))
        os.mkdir(name)
        os.chdir(name)



def find_book(name):
    global book_url

    print('正在书库中查找 <%s> 请稍后...'%name)
    url1 = 'https://www.bqg3.com/quanbuxiaoshuo/'
    soup = BeautifulSoup(get(url1),'lxml')
    all_book = soup.find('div', id='main').find_all('a')
    for i in all_book:
        if i.contents[0] ==name:
            book_url = i.get('href')
            print(' 小说URL地址: ',book_url)
            break
    else:
        print('''# 该小说未找到或网站未收录!!! #\n# 请确认书名是否输入正确!     #\n# 重试或搜索别的小说试试吧!   #''')

if __name__ == '__main__':
    print("已开始，请耐心等待")
    book_name = input('请输入要下载的小说名称:')
    new_dir(name=book_name)
    main(book_name)
```

### 新建脚本

- 进入 python ai 这个 app

- 点击右上角的+号新建文件climbingFiction.py
![fiction_1](https://kamchan.oss-cn-shenzhen.aliyuncs.com/technology/python/fiction_1.PNG)

- 然后打开climbingFiction.py把找到的代码粘贴上去 点击右上角的三角形运行
![fiction_2](https://kamchan.oss-cn-shenzhen.aliyuncs.com/technology/python/fiction_2.PNG)

### 运行脚本

- 运行后就会启动脚本 要你输入要查找的小说名称
![fiction_3](https://kamchan.oss-cn-shenzhen.aliyuncs.com/technology/python/fiction_3.PNG)

- 输入名称后点发送 就会开始爬取小说
![fiction_4](https://kamchan.oss-cn-shenzhen.aliyuncs.com/technology/python/fiction_4.PNG)

### 爬去完成

- 爬去完成后点击右上角的文件夹 回到最开始的页面 就会发现多了个小说名称命名的文件夹 爬到的小说就在里面了
![fiction_5](https://kamchan.oss-cn-shenzhen.aliyuncs.com/technology/python/fiction_5.PNG)

### 运行报错

:::warning 遇到报错
首次安装python ai 的小伙伴此时会报错，因为脚本调用的一些模块没有安装，看报错信息的最后一行就可以，引号中就是缺少的模块名，我因为安装过了，这里直接说吧，这个小说脚本一般会提示缺"lxml"和"bs4"。配置方法如下：
:::

### 解决方案

- python ai 主页选择 test-sample文件夹
![fiction_6](https://kamchan.oss-cn-shenzhen.aliyuncs.com/technology/python/fiction_6.PNG)

- 打开里面的test-pip.py文件
![fiction_7](https://kamchan.oss-cn-shenzhen.aliyuncs.com/technology/python/fiction_7.PNG)

- 把脚本中引号里的字母改成你想要添加的模块，缺啥补啥，这里我们补lxml 和 beautiful soup4
![fiction_8](https://kamchan.oss-cn-shenzhen.aliyuncs.com/technology/python/fiction_8.PNG)

- 点右上方小三角运行，等待控制台提示已结束安装即可。同理安装lxml，这里不再截图。你也可以以后这样安装任何脚本提示你缺少的东西。

- 不缺模块了，回去重新运行爬小说脚本。就不会报错了 可以正常运行了


### 依葫芦画瓢 爬图片
```python
# -*- coding=utf-8 -*-
import time
import requests
import re
import os
from bs4 import BeautifulSoup
####################################
url = 'https://www.mzitu.com/all' 
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:70.0) Gecko/20100101 Firefox/70.0',
           'Accept': "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
           'Accept-Encoding': 'gzip',
            "Referer": "https://www.mzitu.com/all"
           }    # 创建头部信息
def get(url):      #发送网络请求
    a= requests.get(url,headers=headers)
    html = a.text
    return html
 
def main():
    soup = BeautifulSoup(get(url),'lxml')  
    all_url = soup.find('div',class_='all').find_all('a') 
    for mulu in all_url:  
        if mulu.get_text() == '早期图片':
            continue
        else:
            result = {
                'title': mulu.get_text(),
                'link': mulu.get('href'),
                'ID': re.findall('\d+', mulu.get('href'))
            }  
        mulu_url = result['link']
        print('读取图帖链接:', mulu_url)
        soup2 = BeautifulSoup(get(mulu_url), 'lxml')   
        img_mulu = soup2.find("div", {"class": "main-image"}).find("img")['src']       
        page = soup2.find_all("span")[9]        
        max_page = page.get_text()
        new_dir(result['title'])
        for j in range(1,int(max_page) + 1):
            next_img_page = mulu_url + '/' + str(j)
            img_html = BeautifulSoup(get(next_img_page), 'lxml')
            #图片链接
            img_url = img_html.find("div", {"class": "main-image"}).find("img")['src']
            #图片名
            img_name = result['title']+str(j)
            # 下载图片
            down(img_name,img_url)
            print('图片地址: ',img_url)
            time.sleep(yanshi)
 
def down(name,image):
    f = open(name + '.jpg','wb+')
    img = requests.get(image,headers=headers)
    if str(img) == '<Response [200]>':
        print('下载图片...',end='')
        f.write(img.content)
    f.close()
 
def new_dir(name):  #创建文件夹
    if os.path.exists(name):
        print('文件夹已存在')
        os.chdir(name)
    else:
        print('创建文件夹: {}'.format(name))
        os.mkdir(name)
        os.chdir(name)
 
if __name__ == '__main__':
    new_dir('Pictrues')        #设定存储爬取图片的路径
    yanshi = 0.5            #设定抓取图片延时(0.5秒)
    main()
 
 
#####end######
```